{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b31749f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      " [[0.01667825 0.01667825 0.01667825 0.01667825 0.01667825 0.01667825\n",
      "  0.01667825 0.01667825 0.01667825 0.01667825 0.01667825 0.01667825\n",
      "  0.01667825 0.01667825 0.01667825 0.01667825 0.01667825 0.01667825\n",
      "  0.01667825 0.01667825 0.03335651 0.01667825 0.01667825 0.03335651\n",
      "  0.18346078 0.01667825 0.01667825 0.01667825 0.01667825 0.23349554\n",
      "  0.01667825 0.15010428 0.01667825 0.01667825 0.01667825 0.01667825\n",
      "  0.01667825 0.01667825 0.01667825 0.01667825 0.01667825 0.03335651\n",
      "  0.08339126 0.03335651 0.01667825 0.01667825 0.01667825 0.03335651\n",
      "  0.01667825 0.01667825 0.01667825 0.01667825 0.01667825 0.01667825\n",
      "  0.01667825 0.01667825 0.01667825 0.01667825 0.01667825 0.01667825\n",
      "  0.01667825 0.01667825 0.01667825 0.01667825 0.01667825 0.01667825\n",
      "  0.01667825 0.01667825 0.01667825 0.01667825 0.01667825 0.03335651\n",
      "  0.01667825 0.01667825 0.01667825 0.01667825 0.01667825 0.01667825\n",
      "  0.01667825 0.01667825 0.01667825 0.01667825 0.01667825 0.01667825\n",
      "  0.03335651 0.01667825 0.01667825 0.08339126 0.01667825 0.01667825\n",
      "  0.01667825 0.01667825 0.01667825 0.03335651 0.01667825 0.01667825\n",
      "  0.01667825 0.01667825 0.01667825 0.03335651 0.08339126 0.01667825\n",
      "  0.01667825 0.01667825 0.03335651 0.01667825 0.03335651 0.05003476\n",
      "  0.11674777 0.03335651 0.01667825 0.01667825 0.01667825 0.01667825\n",
      "  0.01667825 0.03335651 0.01667825 0.05003476 0.01667825 0.03335651\n",
      "  0.01667825 0.01667825 0.01667825 0.01667825 0.01667825 0.26685204\n",
      "  0.01667825 0.01667825 0.01667825 0.01667825 0.06671301 0.01667825\n",
      "  0.01667825 0.01667825 0.01667825 0.01667825 0.01667825 0.06671301\n",
      "  0.01667825 0.01667825 0.01667825 0.01667825 0.01667825 0.03335651\n",
      "  0.01667825 0.05003476 0.01667825 0.01667825 0.01667825 0.01667825\n",
      "  0.01667825 0.01667825 0.01667825 0.01667825 0.01667825 0.01667825\n",
      "  0.03335651 0.01667825 0.01667825 0.01667825 0.03335651 0.01667825\n",
      "  0.05003476 0.03335651 0.01667825 0.01667825 0.01667825 0.01667825\n",
      "  0.03335651 0.01667825 0.01667825 0.01667825 0.01667825 0.20013903\n",
      "  0.01667825 0.01667825 0.03335651 0.03335651 0.03335651 0.05003476\n",
      "  0.01667825 0.01667825 0.03335651 0.01667825 0.01667825 0.01667825\n",
      "  0.01667825 0.01667825 0.01667825 0.01667825 0.01667825 0.01667825\n",
      "  0.03335651 0.01667825 0.01667825 0.01667825 0.01667825 0.01667825\n",
      "  0.01667825 0.01667825 0.01667825 0.01667825 0.01667825 0.01667825\n",
      "  0.03335651 0.01667825 0.01667825 0.01667825 0.01667825 0.01667825\n",
      "  0.01667825 0.01667825 0.01667825 0.01667825 0.01667825 0.01667825\n",
      "  0.01667825 0.06671301 0.01667825 0.01667825 0.01667825 0.01667825\n",
      "  0.01667825 0.01667825 0.01667825 0.01667825 0.01667825 0.03335651\n",
      "  0.3168868  0.01667825 0.21681729 0.01667825 0.01667825 0.01667825\n",
      "  0.01667825 0.01667825 0.01667825 0.01667825 0.01667825 0.01667825\n",
      "  0.03335651 0.01667825 0.01667825 0.05003476 0.01667825 0.01667825\n",
      "  0.05003476 0.01667825 0.01667825 0.01667825 0.11674777 0.61709535\n",
      "  0.05003476 0.01667825 0.01667825 0.05003476 0.03335651 0.01667825\n",
      "  0.03335651 0.01667825 0.01667825 0.01667825 0.21681729 0.01667825\n",
      "  0.01667825 0.01667825 0.01667825 0.01667825 0.01667825 0.01667825\n",
      "  0.01667825 0.03335651 0.01667825 0.01667825 0.11674777 0.01667825\n",
      "  0.01667825 0.03335651 0.01667825 0.06671301 0.03335651 0.01667825\n",
      "  0.01667825 0.05003476 0.01667825 0.01667825]]\n",
      "Feature Names:\n",
      " ['000' '10' '12' '1442' '1569' '16th' '17' '1970' '200' '359' '40' '419'\n",
      " '444' '458' '600' '71' 'activities' 'addition' 'after' 'agaleus' 'ago'\n",
      " 'all' 'although' 'an' 'and' 'apex' 'appeared' 'applied' 'approximately'\n",
      " 'are' 'around' 'as' 'at' 'back' 'batoidea' 'be' 'beckington' 'been'\n",
      " 'behaviour' 'being' 'both' 'bull' 'by' 'came' 'can' 'card' 'caribbean'\n",
      " 'cartilaginous' 'category' 'caught' 'centimetres' 'century' 'cf' 'chain'\n",
      " 'characterized' 'chondrichthyan' 'chondrichthyans' 'chondrichthyes'\n",
      " 'clade' 'cladoselache' 'classified' 'common' 'confirmed' 'covering'\n",
      " 'damage' 'deep' 'denticles' 'depths' 'derives' 'dermal' 'devonian'\n",
      " 'dictionary' 'disproven' 'do' 'dogfish' 'dogs' 'doliodus' 'due' 'dutch'\n",
      " 'dwarf' 'dynamics' 'earliest' 'early' 'elasmobranch' 'english' 'etc'\n",
      " 'etmopterus' 'etymology' 'evidence' 'evidential' 'examples' 'exceptions'\n",
      " 'exhibited' 'extend' 'extinct' 'far' 'few' 'fin' 'fins' 'first' 'fish'\n",
      " 'five' 'fluid' 'food' 'for' 'fossilized' 'found' 'freshwater' 'from' 'ft'\n",
      " 'fused' 'ganges' 'generally' 'gill' 'great' 'group' 'hammerhead' 'have'\n",
      " 'hawkins' 'head' 'however' 'human' 'humans' 'hybodonts' 'improving' 'in'\n",
      " 'include' 'including' 'informal' 'into' 'is' 'isolated' 'it' 'its' 'john'\n",
      " 'jurassic' 'kin' 'known' 'lanternshark' 'large' 'largest' 'late' 'later'\n",
      " 'length' 'letter' 'like' 'likely' 'live' 'lives' 'loan' 'london' 'mako'\n",
      " 'many' 'mariners' 'may' 'maya' 'meaning' 'meat' 'member' 'members'\n",
      " 'metres' 'middle' 'million' 'modern' 'morphology' 'most' 'mostly' 'new'\n",
      " 'not' 'notes' 'now' 'numerous' 'occurrence' 'of' 'old' 'oldest' 'on'\n",
      " 'one' 'only' 'or' 'ordovician' 'organisms' 'original' 'others' 'out'\n",
      " 'overfishing' 'oxford' 'parasites' 'pectoral' 'period' 'permian' 'perryi'\n",
      " 'populations' 'porbeagle' 'posted' 'predator' 'predators' 'predatory'\n",
      " 'preys' 'pronounced' 'protects' 'range' 'rays' 'reaches' 'records'\n",
      " 'reduced' 'refer' 'referring' 'replaceable' 'research' 'rhincodon'\n",
      " 'river' 'rules' 'sailors' 'scales' 'schurk' 'scoundrel' 'sea' 'seas'\n",
      " 'seawater' 'selachii' 'selachimorpha' 'selachimorphs' 'select' 'sense'\n",
      " 'sets' 'seven' 'several' 'shark' 'sharke' 'sharks' 'sides' 'since' 'sir'\n",
      " 'sister' 'size' 'skeleton' 'skin' 'slits' 'small' 'some' 'soup' 'sources'\n",
      " 'species' 'states' 'still' 'such' 'teeth' 'term' 'termed' 'that' 'the'\n",
      " 'their' 'theory' 'there' 'they' 'this' 'thomas' 'though' 'threatened'\n",
      " 'thresher' 'tiger' 'to' 'top' 'true' 'typus' 'uncertain' 'until' 'up'\n",
      " 'use' 'villain' 'was' 'were' 'whale' 'which' 'white' 'who' 'with'\n",
      " 'within' 'word' 'world' 'written' 'xook' 'years' 'yucatec' 'ʃoːk']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def load_text_files(directory):\n",
    "    \"\"\"\n",
    "    Load text files from a directory and return a list of their contents.\n",
    "    \n",
    "    Parameters:\n",
    "    directory (str): The path to the directory containing text files.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list containing the contents of each text file.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                documents.append(file.read())\n",
    "    return documents\n",
    "\n",
    "def tfidf_vectorize(directory):\n",
    "    \"\"\"\n",
    "    Perform TF-IDF vectorization on text files in a specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "    directory (str): The path to the directory containing text files.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the TF-IDF matrix and the feature names.\n",
    "    \"\"\"\n",
    "    # Load text documents\n",
    "    documents = load_text_files(directory)\n",
    "    \n",
    "    # Initialize the TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Perform TF-IDF vectorization\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    # Get the feature names\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    return tfidf_matrix, feature_names\n",
    "\n",
    "# Example usage:\n",
    "# In this case in the folder 'data', I have a single text file entitled sharks.txt.\n",
    "# You can add more text files to the data folder and vectorize them all\n",
    "\n",
    "directory_path = 'data'\n",
    "tfidf_matrix, feature_names = tfidf_vectorize(directory_path)\n",
    "\n",
    "# Printing the TF-IDF matrix\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n",
    "# Printing the feature names\n",
    "print(\"Feature Names:\\n\", feature_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
